{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5476861",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-19T18:44:19.812116Z",
     "iopub.status.busy": "2022-06-19T18:44:19.811493Z",
     "iopub.status.idle": "2022-06-19T18:44:25.592078Z",
     "shell.execute_reply": "2022-06-19T18:44:25.591436Z",
     "shell.execute_reply.started": "2022-06-19T13:15:25.652019Z"
    },
    "papermill": {
     "duration": 5.801917,
     "end_time": "2022-06-19T18:44:25.592247",
     "exception": false,
     "start_time": "2022-06-19T18:44:19.790330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 18:44:20.518284: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n",
      "2022-06-19 18:44:20.518395: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae61a55b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T18:44:25.617150Z",
     "iopub.status.busy": "2022-06-19T18:44:25.616139Z",
     "iopub.status.idle": "2022-06-19T18:44:31.095231Z",
     "shell.execute_reply": "2022-06-19T18:44:31.094510Z",
     "shell.execute_reply.started": "2022-06-19T13:15:30.859547Z"
    },
    "papermill": {
     "duration": 5.492412,
     "end_time": "2022-06-19T18:44:31.095392",
     "exception": false,
     "start_time": "2022-06-19T18:44:25.602980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 18:44:25.623546: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-19 18:44:25.626681: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n",
      "2022-06-19 18:44:25.626715: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-19 18:44:25.626741: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (508e2ae5058c): /proc/driver/nvidia/version does not exist\n",
      "2022-06-19 18:44:25.629334: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-19 18:44:25.630765: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-19 18:44:25.660862: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n",
      "2022-06-19 18:44:25.660939: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30019}\n",
      "2022-06-19 18:44:25.685804: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n",
      "2022-06-19 18:44:25.685882: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30019}\n",
      "2022-06-19 18:44:25.687391: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30019\n"
     ]
    }
   ],
   "source": [
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "# instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9993c8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T18:44:31.132124Z",
     "iopub.status.busy": "2022-06-19T18:44:31.131043Z",
     "iopub.status.idle": "2022-06-19T18:44:31.406687Z",
     "shell.execute_reply": "2022-06-19T18:44:31.405580Z",
     "shell.execute_reply.started": "2022-06-19T13:15:36.769830Z"
    },
    "papermill": {
     "duration": 0.300284,
     "end_time": "2022-06-19T18:44:31.406880",
     "exception": false,
     "start_time": "2022-06-19T18:44:31.106596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text1 = open('../input/mydata/Crimson Sabre.txt',encoding='utf-8').read()\n",
    "text2= open('../input/mydata/Island of No return-Xia Ko Shin-Hap Kak Hang.txt', encoding='utf-8').read()\n",
    "text3 =open('../input/mydata/SheDioYiXiongChuan.txt',encoding='utf-8').read()\n",
    "text4 =open('../input/mydata/TeinLongBaBu.txt',encoding='utf-8').read()\n",
    "text5 = open('../input/mydata/The Flying Fox of Snowy Mountain.txt',encoding='utf-8').read()\n",
    "text6= open('../input/mydata/White Horse Neighing in the West Wind.txt', encoding='utf-8').read()\n",
    "text7 =open('../input/mydata/XIaoaoginhu.txt',encoding='utf-8').read()\n",
    "text8 =open('../input/mydata/yiteintulonggy.txt',encoding='utf-8').read()\n",
    "text= text1 + text2 +text3 +text4+ text5 + text6 +text7 +text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f36857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T18:44:31.835227Z",
     "iopub.status.busy": "2022-06-19T18:44:31.830047Z",
     "iopub.status.idle": "2022-06-19T18:44:40.714420Z",
     "shell.execute_reply": "2022-06-19T18:44:40.713810Z",
     "shell.execute_reply.started": "2022-06-19T13:15:37.076671Z"
    },
    "papermill": {
     "duration": 9.296652,
     "end_time": "2022-06-19T18:44:40.714570",
     "exception": false,
     "start_time": "2022-06-19T18:44:31.417918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = len(set(text))\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=w,char_level=True,filters='')\n",
    "tokenizer.fit_on_texts(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4459718a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T18:44:40.794015Z",
     "iopub.status.busy": "2022-06-19T18:44:40.793021Z",
     "iopub.status.idle": "2022-06-19T18:45:00.502495Z",
     "shell.execute_reply": "2022-06-19T18:45:00.503533Z",
     "shell.execute_reply.started": "2022-06-19T13:15:46.302733Z"
    },
    "papermill": {
     "duration": 19.77746,
     "end_time": "2022-06-19T18:45:00.503860",
     "exception": false,
     "start_time": "2022-06-19T18:44:40.726400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
      "<BatchDataset shapes: (11,), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "# 方便說明，實際上我們會用更大的值來讓模型從更長的序列預測下個中文字\n",
    "SEQ_LENGTH = 10  # 數字序列長度\n",
    "BATCH_SIZE = 128 # 幾筆成對輸入/輸出\n",
    "text_as_int = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# 我們利用 from_tensor_slices 將其轉變成 TensorFlow 最愛的 Tensor\n",
    "characters = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "print(type(characters))\n",
    "# 將被以數字序列表示的天龍八部文本拆成多個長度為 (SEQ_LENGTH(10)+1) 的序列\n",
    "# 並將最後長度不滿 SEQ_LENGTH 的序列捨去\n",
    "sequences = characters.batch(SEQ_LENGTH + 1,drop_remainder=True)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4fb3064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T18:45:00.531616Z",
     "iopub.status.busy": "2022-06-19T18:45:00.530570Z",
     "iopub.status.idle": "2022-06-19T18:45:00.625084Z",
     "shell.execute_reply": "2022-06-19T18:45:00.625605Z",
     "shell.execute_reply.started": "2022-06-19T13:16:05.526524Z"
    },
    "papermill": {
     "duration": 0.109745,
     "end_time": "2022-06-19T18:45:00.625774",
     "exception": false,
     "start_time": "2022-06-19T18:45:00.516029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成對輸入輸出數量: 518395\n",
      "ds.map: <bound method DatasetV2.map of <BatchDataset shapes: ((128, 10), (128, 10)), types: (tf.int32, tf.int32)>>\n",
      "ds.map 取第一個值： <TakeDataset shapes: ((128, 10), (128, 10)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# 天龍八部全文所包含的成對輸入/輸出的數量\n",
    "steps_per_epoch = \\\n",
    "    len(text_as_int) // SEQ_LENGTH\n",
    "print(\"成對輸入輸出數量:\",steps_per_epoch)\n",
    "# 成對輸入輸出數量 414632 （4146323/10=414632）\n",
    "\n",
    "# 這個函式專門負責把一個序列拆成兩個序列，分別代表輸入與輸出\n",
    "def build_seq_pairs(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "# 將每個從文本擷取出來的序列套用上面定義的函式，拆成兩個數字序列\n",
    "# 作為輸入／輸出序列再將得到的所有數據隨機打亂順序最後再一次拿出 BATCH_SIZE（128）筆數據\n",
    "# ds作為模型一次訓練步驟的所使用的資料\n",
    "ds = sequences\\\n",
    "    .map(build_seq_pairs)\\\n",
    "    .shuffle(steps_per_epoch)\\\n",
    "    .batch(BATCH_SIZE, \n",
    "           drop_remainder=True)\n",
    "\n",
    "print(\"ds.map:\",ds.map)\n",
    "print(\"ds.map 取第一個值：\",ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de6093b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T18:45:00.654043Z",
     "iopub.status.busy": "2022-06-19T18:45:00.652918Z",
     "iopub.status.idle": "2022-06-19T18:45:00.655045Z",
     "shell.execute_reply": "2022-06-19T18:45:00.655574Z",
     "shell.execute_reply.started": "2022-06-19T13:16:05.619596Z"
    },
    "papermill": {
     "duration": 0.01864,
     "end_time": "2022-06-19T18:45:00.655737",
     "exception": false,
     "start_time": "2022-06-19T18:45:00.637097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 超參數\n",
    "EMBEDDING_DIM = 512\n",
    "RNN_UNITS = 1024\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9963c9da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T18:45:00.682707Z",
     "iopub.status.busy": "2022-06-19T18:45:00.681645Z",
     "iopub.status.idle": "2022-06-19T18:45:01.664354Z",
     "shell.execute_reply": "2022-06-19T18:45:01.663697Z",
     "shell.execute_reply.started": "2022-06-19T13:16:05.627441Z"
    },
    "papermill": {
     "duration": 0.997262,
     "end_time": "2022-06-19T18:45:01.664501",
     "exception": false,
     "start_time": "2022-06-19T18:45:00.667239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instantiating the model in the strategy scope creates the model on the TPU\n",
    "with tpu_strategy.scope():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=w,output_dim=EMBEDDING_DIM,batch_input_shape=[BATCH_SIZE, None]))\n",
    "\n",
    "    # LSTM層，負責將序列數據依序讀入並做處理  原本在GPU上是設stateful=true, TPU上設為空\n",
    "    model.add(tf.keras.layers.LSTM(units=RNN_UNITS, return_sequences=True, stateful='', recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(w))\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d9d05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T18:45:01.694354Z",
     "iopub.status.busy": "2022-06-19T18:45:01.693411Z",
     "iopub.status.idle": "2022-06-19T22:29:15.030351Z",
     "shell.execute_reply": "2022-06-19T22:29:15.031011Z",
     "shell.execute_reply.started": "2022-06-19T13:16:06.605995Z"
    },
    "papermill": {
     "duration": 13453.355164,
     "end_time": "2022-06-19T22:29:15.031273",
     "exception": false,
     "start_time": "2022-06-19T18:45:01.676109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 5.0616\n",
      "Epoch 2/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 3.7690\n",
      "Epoch 3/200\n",
      "3681/3681 [==============================] - 62s 13ms/step - loss: 3.5094\n",
      "Epoch 4/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 3.3540\n",
      "Epoch 5/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 3.2299\n",
      "Epoch 6/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 3.1293\n",
      "Epoch 7/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 3.0370\n",
      "Epoch 8/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.9611\n",
      "Epoch 9/200\n",
      "3681/3681 [==============================] - 61s 13ms/step - loss: 2.8919\n",
      "Epoch 10/200\n",
      "3681/3681 [==============================] - 62s 13ms/step - loss: 2.8268\n",
      "Epoch 11/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.7729\n",
      "Epoch 12/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.7218\n",
      "Epoch 13/200\n",
      "3681/3681 [==============================] - 62s 13ms/step - loss: 2.6792\n",
      "Epoch 14/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.6387\n",
      "Epoch 15/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.6013\n",
      "Epoch 16/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.5719\n",
      "Epoch 17/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.5441\n",
      "Epoch 18/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.5191\n",
      "Epoch 19/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.4965\n",
      "Epoch 20/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.4776\n",
      "Epoch 21/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.4593\n",
      "Epoch 22/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.4433\n",
      "Epoch 23/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.4280\n",
      "Epoch 24/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.4177\n",
      "Epoch 25/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.4062\n",
      "Epoch 26/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.3985\n",
      "Epoch 27/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.3864\n",
      "Epoch 28/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.3800\n",
      "Epoch 29/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.3761\n",
      "Epoch 30/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.3687\n",
      "Epoch 31/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.3646\n",
      "Epoch 32/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.3565\n",
      "Epoch 33/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.3541\n",
      "Epoch 34/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.3511\n",
      "Epoch 35/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.3494\n",
      "Epoch 36/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.3447\n",
      "Epoch 37/200\n",
      "3681/3681 [==============================] - 73s 14ms/step - loss: 2.3441\n",
      "Epoch 38/200\n",
      "3681/3681 [==============================] - 64s 14ms/step - loss: 2.3440\n",
      "Epoch 39/200\n",
      "3681/3681 [==============================] - 68s 14ms/step - loss: 2.3405\n",
      "Epoch 40/200\n",
      "3681/3681 [==============================] - 68s 14ms/step - loss: 2.3411\n",
      "Epoch 41/200\n",
      "3681/3681 [==============================] - 68s 14ms/step - loss: 2.3417\n",
      "Epoch 42/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.3440\n",
      "Epoch 43/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.3432\n",
      "Epoch 44/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.3428\n",
      "Epoch 45/200\n",
      "3681/3681 [==============================] - 72s 14ms/step - loss: 2.3430\n",
      "Epoch 46/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.3469\n",
      "Epoch 47/200\n",
      "3681/3681 [==============================] - 66s 14ms/step - loss: 2.3458\n",
      "Epoch 48/200\n",
      "3681/3681 [==============================] - 65s 14ms/step - loss: 2.3464\n",
      "Epoch 49/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.3481\n",
      "Epoch 50/200\n",
      "3681/3681 [==============================] - 72s 13ms/step - loss: 2.3517\n",
      "Epoch 51/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.3518\n",
      "Epoch 52/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.3559\n",
      "Epoch 53/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.3588\n",
      "Epoch 54/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.3592\n",
      "Epoch 55/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.3637\n",
      "Epoch 56/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.3662\n",
      "Epoch 57/200\n",
      "3681/3681 [==============================] - 62s 13ms/step - loss: 2.3666\n",
      "Epoch 58/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.3707\n",
      "Epoch 59/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.3760\n",
      "Epoch 60/200\n",
      "3681/3681 [==============================] - 62s 13ms/step - loss: 2.3761\n",
      "Epoch 61/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.3797\n",
      "Epoch 62/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.3823\n",
      "Epoch 63/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.3869\n",
      "Epoch 64/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.3885\n",
      "Epoch 65/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.3896\n",
      "Epoch 66/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.3969\n",
      "Epoch 67/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.4010\n",
      "Epoch 68/200\n",
      "3681/3681 [==============================] - 61s 13ms/step - loss: 2.4021\n",
      "Epoch 69/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.4052\n",
      "Epoch 70/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.4110\n",
      "Epoch 71/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.4124\n",
      "Epoch 72/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.4135\n",
      "Epoch 73/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.4205\n",
      "Epoch 74/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.4261\n",
      "Epoch 75/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.4278\n",
      "Epoch 76/200\n",
      "3681/3681 [==============================] - 69s 14ms/step - loss: 2.4338\n",
      "Epoch 77/200\n",
      "3681/3681 [==============================] - 73s 14ms/step - loss: 2.4344\n",
      "Epoch 78/200\n",
      "3681/3681 [==============================] - 70s 14ms/step - loss: 2.4410\n",
      "Epoch 79/200\n",
      "3681/3681 [==============================] - 70s 14ms/step - loss: 2.4421\n",
      "Epoch 80/200\n",
      "3681/3681 [==============================] - 71s 14ms/step - loss: 2.4474\n",
      "Epoch 81/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.4522\n",
      "Epoch 82/200\n",
      "3681/3681 [==============================] - 66s 14ms/step - loss: 2.4558\n",
      "Epoch 83/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.4590\n",
      "Epoch 84/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.4628\n",
      "Epoch 85/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.4679\n",
      "Epoch 86/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.4718\n",
      "Epoch 87/200\n",
      "3681/3681 [==============================] - 62s 13ms/step - loss: 2.4754\n",
      "Epoch 88/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.4801\n",
      "Epoch 89/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.4848\n",
      "Epoch 90/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.4887\n",
      "Epoch 91/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.4932\n",
      "Epoch 92/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.4956\n",
      "Epoch 93/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.5003\n",
      "Epoch 94/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.5039\n",
      "Epoch 95/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.5098\n",
      "Epoch 96/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.5134\n",
      "Epoch 97/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.5175\n",
      "Epoch 98/200\n",
      "3681/3681 [==============================] - 61s 13ms/step - loss: 2.5198\n",
      "Epoch 99/200\n",
      "3681/3681 [==============================] - 62s 13ms/step - loss: 2.5257\n",
      "Epoch 100/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.5286\n",
      "Epoch 101/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.5322\n",
      "Epoch 102/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.5377\n",
      "Epoch 103/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.5391\n",
      "Epoch 104/200\n",
      "3681/3681 [==============================] - 61s 13ms/step - loss: 2.5437\n",
      "Epoch 105/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.5463\n",
      "Epoch 106/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.5525\n",
      "Epoch 107/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.5550\n",
      "Epoch 108/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.5619\n",
      "Epoch 109/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.5613\n",
      "Epoch 110/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.5683\n",
      "Epoch 111/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.5704\n",
      "Epoch 112/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.5777\n",
      "Epoch 113/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.5803\n",
      "Epoch 114/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.5823\n",
      "Epoch 115/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.5872\n",
      "Epoch 116/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.5903\n",
      "Epoch 117/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.5959\n",
      "Epoch 118/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.5981\n",
      "Epoch 119/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.6018\n",
      "Epoch 120/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.6052\n",
      "Epoch 121/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.6097\n",
      "Epoch 122/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.6145\n",
      "Epoch 123/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.6147\n",
      "Epoch 124/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.6211\n",
      "Epoch 125/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.6237\n",
      "Epoch 126/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.6277\n",
      "Epoch 127/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.6323\n",
      "Epoch 128/200\n",
      "3681/3681 [==============================] - 72s 13ms/step - loss: 2.6359\n",
      "Epoch 129/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.6385\n",
      "Epoch 130/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.6442\n",
      "Epoch 131/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.6470\n",
      "Epoch 132/200\n",
      "3681/3681 [==============================] - 68s 14ms/step - loss: 2.6519\n",
      "Epoch 133/200\n",
      "3681/3681 [==============================] - 71s 14ms/step - loss: 2.6547\n",
      "Epoch 134/200\n",
      "3681/3681 [==============================] - 72s 14ms/step - loss: 2.6575\n",
      "Epoch 135/200\n",
      "3681/3681 [==============================] - 68s 14ms/step - loss: 2.6619\n",
      "Epoch 136/200\n",
      "3681/3681 [==============================] - 66s 14ms/step - loss: 2.6648\n",
      "Epoch 137/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.6654\n",
      "Epoch 138/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.6688\n",
      "Epoch 139/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.6722\n",
      "Epoch 140/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.6738\n",
      "Epoch 141/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.6784\n",
      "Epoch 142/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.6795\n",
      "Epoch 143/200\n",
      "3681/3681 [==============================] - 72s 13ms/step - loss: 2.6875\n",
      "Epoch 144/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.6854\n",
      "Epoch 145/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.6918\n",
      "Epoch 146/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.6937\n",
      "Epoch 147/200\n",
      "3681/3681 [==============================] - 71s 14ms/step - loss: 2.6969\n",
      "Epoch 148/200\n",
      "3681/3681 [==============================] - 68s 14ms/step - loss: 2.7012\n",
      "Epoch 149/200\n",
      "3681/3681 [==============================] - 68s 14ms/step - loss: 2.7058\n",
      "Epoch 150/200\n",
      "3681/3681 [==============================] - 65s 14ms/step - loss: 2.7080\n",
      "Epoch 151/200\n",
      "3681/3681 [==============================] - 69s 14ms/step - loss: 2.7104\n",
      "Epoch 152/200\n",
      "3681/3681 [==============================] - 66s 14ms/step - loss: 2.7127\n",
      "Epoch 153/200\n",
      "3681/3681 [==============================] - 64s 14ms/step - loss: 2.7175\n",
      "Epoch 154/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.7199\n",
      "Epoch 155/200\n",
      "3681/3681 [==============================] - 72s 13ms/step - loss: 2.7231\n",
      "Epoch 156/200\n",
      "3681/3681 [==============================] - 62s 13ms/step - loss: 2.7277\n",
      "Epoch 157/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.7311\n",
      "Epoch 158/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.7327\n",
      "Epoch 159/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.7353\n",
      "Epoch 160/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.7368\n",
      "Epoch 161/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.7404\n",
      "Epoch 162/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.7438\n",
      "Epoch 163/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.7506\n",
      "Epoch 164/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.7522\n",
      "Epoch 165/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.7533\n",
      "Epoch 166/200\n",
      "3681/3681 [==============================] - 71s 13ms/step - loss: 2.7557\n",
      "Epoch 167/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.7591\n",
      "Epoch 168/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.7591\n",
      "Epoch 169/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.7648\n",
      "Epoch 170/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.7668\n",
      "Epoch 171/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.7663\n",
      "Epoch 172/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.7702\n",
      "Epoch 173/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.7762\n",
      "Epoch 174/200\n",
      "3681/3681 [==============================] - 67s 13ms/step - loss: 2.7773\n",
      "Epoch 175/200\n",
      "3681/3681 [==============================] - 61s 13ms/step - loss: 2.7803\n",
      "Epoch 176/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.7802\n",
      "Epoch 177/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.7822\n",
      "Epoch 178/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.7871\n",
      "Epoch 179/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.7894\n",
      "Epoch 180/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.7906\n",
      "Epoch 181/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.7932\n",
      "Epoch 182/200\n",
      "3681/3681 [==============================] - 62s 13ms/step - loss: 2.7975\n",
      "Epoch 183/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.7993\n",
      "Epoch 184/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.8011\n",
      "Epoch 185/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.8033\n",
      "Epoch 186/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.8064\n",
      "Epoch 187/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.8053\n",
      "Epoch 188/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.8104\n",
      "Epoch 189/200\n",
      "3681/3681 [==============================] - 65s 13ms/step - loss: 2.8147\n",
      "Epoch 190/200\n",
      "3681/3681 [==============================] - 61s 13ms/step - loss: 2.8161\n",
      "Epoch 191/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.8160\n",
      "Epoch 192/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.8195\n",
      "Epoch 193/200\n",
      "3681/3681 [==============================] - 64s 13ms/step - loss: 2.8215\n",
      "Epoch 194/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.8236\n",
      "Epoch 195/200\n",
      "3681/3681 [==============================] - 63s 13ms/step - loss: 2.8278\n",
      "Epoch 196/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.8305\n",
      "Epoch 197/200\n",
      "3681/3681 [==============================] - 66s 13ms/step - loss: 2.8300\n",
      "Epoch 198/200\n",
      "3681/3681 [==============================] - 70s 13ms/step - loss: 2.8318\n",
      "Epoch 199/200\n",
      "3681/3681 [==============================] - 68s 13ms/step - loss: 2.8383\n",
      "Epoch 200/200\n",
      "3681/3681 [==============================] - 69s 13ms/step - loss: 2.8394\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200 # 決定看幾篇天龍八部文本\n",
    "history = model.fit(\n",
    "    ds, # 前面使用 tf.data 建構的資料集\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d4df54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T22:31:07.834399Z",
     "iopub.status.busy": "2022-06-19T22:31:07.833128Z",
     "iopub.status.idle": "2022-06-19T22:31:08.587046Z",
     "shell.execute_reply": "2022-06-19T22:31:08.586351Z",
     "shell.execute_reply.started": "2022-06-19T18:43:24.780921Z"
    },
    "papermill": {
     "duration": 57.478904,
     "end_time": "2022-06-19T22:31:08.587219",
     "exception": false,
     "start_time": "2022-06-19T22:30:11.108315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"myModel_8books_10Words_200Epochs.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13676.951085,
   "end_time": "2022-06-19T22:32:08.423457",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-19T18:44:11.472372",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
